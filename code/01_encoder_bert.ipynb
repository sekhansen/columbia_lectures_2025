{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85f4fbce-08ba-456f-8f39-be745a10ec4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /databricks/python3/lib/python3.12/site-packages (2.5.1+cu124)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /databricks/python3/lib/python3.12/site-packages (from torch) (4.11.0)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.12/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.12/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /databricks/python3/lib/python3.12/site-packages (from torch) (2023.5.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /databricks/python3/lib/python3.12/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /databricks/python3/lib/python3.12/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /databricks/python3/lib/python3.12/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /databricks/python3/lib/python3.12/site-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /databricks/python3/lib/python3.12/site-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /databricks/python3/lib/python3.12/site-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /databricks/python3/lib/python3.12/site-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /databricks/python3/lib/python3.12/site-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /databricks/python3/lib/python3.12/site-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /databricks/python3/lib/python3.12/site-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /databricks/python3/lib/python3.12/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /databricks/python3/lib/python3.12/site-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /databricks/python3/lib/python3.12/site-packages (from torch) (3.1.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (74.0.0)\nRequirement already satisfied: sympy==1.13.1 in /databricks/python3/lib/python3.12/site-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /databricks/python3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c74fe53-3a77-487c-bf59-177f98d52642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import BertTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8cb6f95-327e-4978-9007-b2a58a7f3167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nDevice set to use cuda:0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'score': 0.4774385690689087,\n",
       "  'token': 3509,\n",
       "  'token_str': 'beach',\n",
       "  'sequence': 'every morning last summer in greece, i visited the beach where i would swim, play in the sand, and sunbathe.'},\n",
       " {'score': 0.22596196830272675,\n",
       "  'token': 12212,\n",
       "  'token_str': 'beaches',\n",
       "  'sequence': 'every morning last summer in greece, i visited the beaches where i would swim, play in the sand, and sunbathe.'},\n",
       " {'score': 0.05403747782111168,\n",
       "  'token': 4770,\n",
       "  'token_str': 'pool',\n",
       "  'sequence': 'every morning last summer in greece, i visited the pool where i would swim, play in the sand, and sunbathe.'},\n",
       " {'score': 0.04365471750497818,\n",
       "  'token': 12679,\n",
       "  'token_str': 'pools',\n",
       "  'sequence': 'every morning last summer in greece, i visited the pools where i would swim, play in the sand, and sunbathe.'},\n",
       " {'score': 0.02927646040916443,\n",
       "  'token': 2697,\n",
       "  'token_str': 'lake',\n",
       "  'sequence': 'every morning last summer in greece, i visited the lake where i would swim, play in the sand, and sunbathe.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "test = \"Every morning last summer in Greece, \" + \\\n",
    "       \"I visited the [MASK] where I would swim, \" + \\\n",
    "       \"play in the sand, and sunbathe.\"\n",
    "\n",
    "result = unmasker(test)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aefc923e-c6c2-4646-8133-cf87f3442eb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c9f22d-68ad-479c-9e0d-226463455007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Volumes/daz_aitraining_cat/aitraining/aitraining_volume/fed_sentiment_training.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c23d64a5-3f72-413d-b3bf-8a39ce6714e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>157_1</td>\n",
       "      <td>The action was taken to cushion the effects on...</td>\n",
       "      <td>dovish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>161_2</td>\n",
       "      <td>Such trends could foster inflationary imbalanc...</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52_0</td>\n",
       "      <td>The Federal Open Market Committee at its meeti...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21_5</td>\n",
       "      <td>Although continuing favorable trends bolster l...</td>\n",
       "      <td>dovish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78_7</td>\n",
       "      <td>The Committee perceives that the upside and do...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>115_6</td>\n",
       "      <td>Nonetheless, the Committee judges that some in...</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>160_4</td>\n",
       "      <td>As a consequence, the pool of available worker...</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>114_3</td>\n",
       "      <td>Readings on core inflation have been elevated,...</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>60_0</td>\n",
       "      <td>The Federal Open Market Committee at its meeti...</td>\n",
       "      <td>dovish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>80_7</td>\n",
       "      <td>The Committee judges that, on balance, the ris...</td>\n",
       "      <td>dovish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                               text sentiment\n",
       "0  157_1  The action was taken to cushion the effects on...    dovish\n",
       "1  161_2  Such trends could foster inflationary imbalanc...   hawkish\n",
       "2   52_0  The Federal Open Market Committee at its meeti...   neutral\n",
       "3   21_5  Although continuing favorable trends bolster l...    dovish\n",
       "4   78_7  The Committee perceives that the upside and do...   neutral\n",
       "5  115_6  Nonetheless, the Committee judges that some in...   hawkish\n",
       "6  160_4  As a consequence, the pool of available worker...   hawkish\n",
       "7  114_3  Readings on core inflation have been elevated,...   hawkish\n",
       "8   60_0  The Federal Open Market Committee at its meeti...    dovish\n",
       "9   80_7  The Committee judges that, on balance, the ris...    dovish"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04b240b0-9a91-44c4-b851-2976d4e58494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The action was taken to cushion the effects on prospective economic growth in the United States of increasing weakness in foreign economies and of less accommodative financial conditions domestically'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdb011eb-0c9e-4730-bbbb-ef4d90d44562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tokenizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08d976e9-196a-42e7-85c9-5c671fbdb5fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9d153b1-66da-4ef0-b077-07dc0eb55a51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in vocabulary: 30522 \n---------\nmoments 5312\nterrible 6659\nfate 6580\npersonality 6180\nannoyance 17466\nð 1098\n1729 28449\nsummit 6465\ncm 4642\nconfess 18766\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "print(f\"Total number of tokens in vocabulary: {len(vocab)} \\n---------\")\n",
    "for _ in range(10):\n",
    "    word, idx = random.choice(list(vocab.items()))\n",
    "    print(word, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5ee257e-756d-451c-83c2-fd29e89b8ad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n['[CLS]', 'the', 'action', 'was', 'taken', 'to', 'cushion', 'the', 'effects', 'on', 'prospective', 'economic', 'growth', 'in', 'the', 'united', 'states', 'of', 'increasing', 'weakness', 'in', 'foreign', 'economies', 'and', 'of', 'less', 'acc', '##om', '##mo', '##da', '##tive', 'financial', 'conditions', 'domestically', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n\n------------------------------------------\n\nTokens IDs:\ntensor([  101,  1996,  2895,  2001,  2579,  2000, 22936,  1996,  3896,  2006,\n        17464,  3171,  3930,  1999,  1996,  2142,  2163,  1997,  4852, 11251,\n         1999,  3097, 18730,  1998,  1997,  2625, 16222,  5358,  5302,  2850,\n         6024,  3361,  3785, 27143,   102,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "encoded_input1 = tokenizer(df.loc[0, \"text\"],\n",
    "                           max_length=100,\n",
    "                           padding=\"max_length\",\n",
    "                           return_tensors='pt')\n",
    "\n",
    "print(\"Tokens:\")\n",
    "temp_tokens = encoded_input1[\"input_ids\"][0]  # ← Add [0] here to get first sequence\n",
    "print(tokenizer.convert_ids_to_tokens(temp_tokens))\n",
    "print(\"\\n------------------------------------------\\n\")\n",
    "print(\"Tokens IDs:\")\n",
    "print(temp_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dceb3e85-a502-4483-8eb0-794c25993392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Obtaining Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cfebb8b-6e08-4970-9a8c-288d0f638229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n  \"_attn_implementation_autoset\": true,\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_attentions\": true,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.48.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"bert-base-uncased\",\n",
    "                                  output_hidden_states=True,\n",
    "                                  output_attentions=True,\n",
    "                                  attn_implementation=\"eager\"\n",
    "                                  )\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb6d53ed-95f4-4281-b31f-e3b42b0b52a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embeddings shape: torch.Size([1, 100, 768])\nattention mask:\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0]])\n\n------------------------------------------\n\nSentence embedding shape: torch.Size([1, 768])\n\n------------------------------------------\n\nFirst Ten Elements of Embeddings:\ntensor([-0.7555, -0.1470, -0.0567,  0.2348,  0.1567,  0.0438, -0.0582,  0.4445,\n        -0.1379, -0.1501])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get BERT output without computing gradients (inference mode)\n",
    "with torch.no_grad():\n",
    "    result1 = model(**encoded_input1)\n",
    "\n",
    "# Step 2: Extract token-level embeddings from BERT's last layer\n",
    "last_hidden_state = result1.last_hidden_state\n",
    "print(f\"Token embeddings shape: {last_hidden_state.shape}\")\n",
    "\n",
    "# Step 3: Get attention mask (1 = real token, 0 = padding)\n",
    "attention_mask = encoded_input1[\"attention_mask\"]  # [1, 30]\n",
    "print(\"attention mask:\")\n",
    "print(attention_mask)\n",
    "print(\"\\n------------------------------------------\\n\")\n",
    "\n",
    "# Step 4: Zero out padding token embeddings\n",
    "# unsqueeze(-1) adds dimension: [1, 30] → [1, 30, 1]\n",
    "# This allows broadcasting when multiplying with embeddings [1, 30, 768]\n",
    "masked_embeddings = last_hidden_state * attention_mask.unsqueeze(-1)\n",
    "\n",
    "# Step 5: Compute mean pooling (average of non-padding tokens)\n",
    "# Numerator: sum all 30 token embeddings\n",
    "sum_embeddings = masked_embeddings.sum(dim=1)  # [1, 768]\n",
    "\n",
    "# Denominator: count how many real tokens\n",
    "num_real_tokens = attention_mask.sum(dim=1, keepdim=True)  # [1, 1]\n",
    "\n",
    "# Final sentence embedding: average of real token embeddings\n",
    "mean_embedding1 = sum_embeddings / num_real_tokens  # [1, 768]\n",
    "\n",
    "print(f\"Sentence embedding shape: {mean_embedding1.shape}\")\n",
    "print(\"\\n------------------------------------------\\n\")\n",
    "print(\"First Ten Elements of Embedding:\")\n",
    "print(mean_embedding1[0, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628854c4-2e5c-4154-a778-8976c3c558ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings matrix shape: (1243, 768)\nNumber of texts: 1243\nEmbedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# %% Now scale to ALL examples in the dataset\n",
    "import numpy as np\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "for text in df[\"text\"]:\n",
    "    # Same steps as above\n",
    "    encoded_input = tokenizer(text,\n",
    "                             max_length=30, # for speed\n",
    "                             padding=\"max_length\",\n",
    "                             truncation=True,\n",
    "                             return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        result = model(**encoded_input)\n",
    "    \n",
    "    last_hidden_state = result.last_hidden_state\n",
    "    attention_mask = encoded_input[\"attention_mask\"]\n",
    "    masked_embeddings = last_hidden_state * attention_mask.unsqueeze(-1)\n",
    "    mean_embedding = masked_embeddings.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    embedding_vector = mean_embedding.squeeze(0).numpy()\n",
    "    all_embeddings.append(embedding_vector)\n",
    "\n",
    "# Stack into matrix\n",
    "embeddings_matrix = np.vstack(all_embeddings)\n",
    "\n",
    "print(f\"Embeddings matrix shape: {embeddings_matrix.shape}\")\n",
    "print(f\"Number of texts: {embeddings_matrix.shape[0]}\")\n",
    "print(f\"Embedding dimension: {embeddings_matrix.shape[1]}\")\n",
    "\n",
    "# Store in dataframe\n",
    "df['embedding'] = all_embeddings"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_encoder",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}