{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client with API key from Colab secrets\n",
    "api_key = userdata.get(\"openai_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Call to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to call OpenAI API\n",
    "def call_openai(prompt, system_message=None, model=\"gpt-4o\", temperature=0.1, max_tokens=1500):\n",
    "    messages = []\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"what is the most prestigious university in New York City?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = call_openai(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Structured Representations of Unstructured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by seeing how an LLM can represent the content of news articles. To begin, we use a New Yorker article about Trump's 2024 election victory. We'll load it from a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example article from URL\n",
    "url = 'https://www.dropbox.com/scl/fi/your-file-id/example_text.txt?rlkey=your-key&dl=1'\n",
    "\n",
    "# For demonstration, using a sample text. Replace with your actual article URL or text.\n",
    "ny_article = \"\"\"Donald Trump has won a second term as President of the United States. \n",
    "The Republican candidate defeated Vice President Kamala Harris in a closely watched election. \n",
    "Trump's victory marks a historic return to the White House after losing the 2020 election to Joe Biden.\n",
    "The former President campaigned on themes of economic recovery and immigration reform.\n",
    "Harris, who became the Democratic nominee after President Biden withdrew from the race, \n",
    "focused her campaign on protecting democratic institutions and expanding access to healthcare.\n",
    "The election saw record turnout in several key swing states including Pennsylvania, Georgia, and Arizona.\n",
    "Trump will be inaugurated as the 47th President of the United States in January 2025.\"\"\"\n",
    "\n",
    "print(ny_article[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting information from this raw text requires forming a prompt to the LLM. The study of how to effectively do so is called prompt engineering. For example, OpenAI's guide is at https://platform.openai.com/docs/guides/prompt-engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Be clear and direct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINCIPLE 1: Be clear and detailed (BAD PROMPT)\n",
    "prompt1 = f\"\"\"\n",
    "whom does this article talk about?:\n",
    "\n",
    "{ny_article}\n",
    "\"\"\"\n",
    "\n",
    "print(prompt1[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = call_openai(prompt1, temperature=0.0, max_tokens=5000)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BETTER PROMPT - More specific and structured\n",
    "prompt2 = f\"\"\"\n",
    "We want to extract the relevant people from a news article.\n",
    "\n",
    "Please follow these steps:\n",
    "1. Identify all the people mentioned and any description of them\n",
    "2. Identify any political offices mentioned\n",
    "\n",
    "Here is the text of the article:\n",
    "{ny_article}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = call_openai(prompt2, temperature=0.0, max_tokens=5000)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern LLMs also support prompts that directly incorporate data schema which can help clarify and organize what information you want. Some even have an explicit function mode that guarantees a particular output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRUCTURED JSON OUTPUT\n",
    "prompt3 = f\"\"\"\n",
    "We want to extract the relevant characters from a news article. \n",
    "\n",
    "Please provide your output in the following JSON format:\n",
    "\n",
    "{{\n",
    "  \"people\": [\n",
    "    {{\n",
    "      \"name\": \"person's full name\",\n",
    "      \"description\": \"their role or description from the article\"\n",
    "    }}\n",
    "  ],\n",
    "  \"institutions\": [\n",
    "    {{\n",
    "      \"name\": \"institution name\",\n",
    "      \"type\": \"type of institution (e.g., government, media, party, etc.)\",\n",
    "      \"context\": \"brief context of how it's mentioned\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Here is the text of the article:\n",
    "{ny_article}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = call_openai(prompt3, temperature=0.0, max_tokens=5000)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data schema can fundamentally change how input text is represented: temporal, network, etc. Below we illustrate a network-structured prompt and associated visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK GRAPH REPRESENTATION\n",
    "prompt4 = f\"\"\"\n",
    "Create a network graph representation of the article.\n",
    "\n",
    "Return JSON:\n",
    "\n",
    "{{\n",
    "  \"nodes\": [\n",
    "    {{\"id\": \"trump\", \"label\": \"Donald Trump\", \"type\": \"person\"}},\n",
    "    {{\"id\": \"gop\", \"label\": \"Republican Party\", \"type\": \"institution\"}}\n",
    "  ],\n",
    "  \"edges\": [\n",
    "    {{\"from\": \"trump\", \"to\": \"gop\", \"relationship\": \"leads\"}},\n",
    "    {{\"from\": \"trump\", \"to\": \"harris\", \"relationship\": \"defeated\"}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Article: {ny_article}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = call_openai(prompt4, temperature=0.0, max_tokens=5000)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Get and clean response\n",
    "response = call_openai(prompt4, temperature=0.0, max_tokens=5000)\n",
    "json_str = re.sub(r'^```json\\n|```$', '', response.strip(), flags=re.MULTILINE)\n",
    "data = json.loads(json_str)\n",
    "\n",
    "# Color mapping\n",
    "color_map = {\n",
    "    'person': '#3498db',\n",
    "    'institution': '#2ecc71',\n",
    "    'location': '#f39c12',\n",
    "    'event': '#e74c3c',\n",
    "    'policy': '#9b59b6'\n",
    "}\n",
    "\n",
    "# Create network\n",
    "net = Network(\n",
    "    height='800px', \n",
    "    width='100%', \n",
    "    directed=True, \n",
    "    notebook=True,\n",
    "    cdn_resources='in_line'\n",
    ")\n",
    "\n",
    "# Add nodes\n",
    "for node in data['nodes']:\n",
    "    net.add_node(node['id'], label=node['label'], \n",
    "                 color=color_map.get(node['type'], '#95a5a6'),\n",
    "                 title=f\"{node['label']} ({node['type']})\",\n",
    "                 size=20)\n",
    "\n",
    "# Add edges\n",
    "for edge in data['edges']:\n",
    "    net.add_edge(edge['from'], edge['to'], \n",
    "                 title=edge['relationship'], arrows='to')\n",
    "\n",
    "# Configure and show\n",
    "net.toggle_physics(True)\n",
    "net.show('network.html')\n",
    "\n",
    "print(\"âœ“ Network HTML generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display in Colab\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='network.html', width='100%', height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A system prompt allows you to endow the LLM with a \"persona\" which guides what output is generated. We'll illustrate this with a sample article about Venezuelan gangs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample article for demonstration\n",
    "tt_article = \"\"\"Texas Governor Greg Abbott announced a crackdown on Tren de Aragua,\n",
    "a Venezuelan gang that has allegedly been involved in criminal activities across the state.\n",
    "Law enforcement officials have reported increased gang activity in several major cities.\n",
    "The gang, which originated in Venezuela, is known for human trafficking and violent crimes.\n",
    "Abbott has allocated additional resources to state police agencies to combat the threat.\n",
    "Critics argue that the focus on gang activity may distract from broader immigration reform discussions.\n",
    "Immigrant advocacy groups have expressed concern about potential profiling of Venezuelan migrants.\"\"\"\n",
    "\n",
    "my_prompt = f\"\"\"Summarize the article below.\n",
    "Article:\n",
    "{tt_article}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt example 1: Constrain format\n",
    "system_message1 = \"You are a helpful assistant that replies with a concise one-sentence answer that always starts with the letter T.\"\n",
    "\n",
    "response = call_openai(my_prompt, system_message=system_message1, temperature=0.0, max_tokens=100)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt example 2: Content filtering\n",
    "system_message2 = \"\"\"You are a language model that works with young children. \n",
    "Never produce content related to violence or gangs. \n",
    "If asked to produce this content please reply with the phrase 'I can't do that :( \\nViolence is not good'\"\"\"\n",
    "\n",
    "response = call_openai(my_prompt, system_message=system_message2, temperature=0.0, max_tokens=100)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
